[2024-12-17 14:54:04,879][1079745] Saving configuration to ./train_dir/paper_quads_multi_mix_baseline_8a_attn_v116/single_/03_single_see_3333/config.json...
[2024-12-17 14:54:04,890][1079745] Rollout worker 0 uses device cpu
[2024-12-17 14:54:04,891][1079745] Rollout worker 1 uses device cpu
[2024-12-17 14:54:04,905][1079745] Using GPUs [0] for process 0 (actually maps to GPUs [3])
[2024-12-17 14:54:04,905][1079745] InferenceWorker_p0-w0: min num requests: 1
[2024-12-17 14:54:04,911][1079745] Starting all processes...
[2024-12-17 14:54:04,911][1079745] Starting process learner_proc0
[2024-12-17 14:54:05,054][1079745] Starting all processes...
[2024-12-17 14:54:05,058][1079745] Starting process inference_proc0-0
[2024-12-17 14:54:05,059][1079745] Starting process rollout_proc0
[2024-12-17 14:54:05,059][1079745] Starting process rollout_proc1
[2024-12-17 14:54:06,633][1080598] Using GPUs [0] for process 0 (actually maps to GPUs [3])
[2024-12-17 14:54:06,634][1080598] Set environment var CUDA_VISIBLE_DEVICES to '3' (GPU indices [0]) for learning process 0
[2024-12-17 14:54:06,652][1080598] Num visible devices: 0
[2024-12-17 14:54:06,696][1080598] WARNING! It is generally recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks to avoid potential numerical issues. I.e. set --kl_loss_coeff=0.1
[2024-12-17 14:54:06,696][1080598] Setting fixed seed 3333
[2024-12-17 14:54:06,697][1080598] Using GPUs [0] for process 0 (actually maps to GPUs [3])
[2024-12-17 14:54:06,697][1080598] Initializing actor-critic model on device cuda:0
[2024-12-17 14:54:06,704][1080598] Created Actor Critic model with architecture:
[2024-12-17 14:54:06,704][1080598] ActorCriticSeparateWeights(
  (obs_normalizer): ObservationNormalizer()
  (actor_encoder): QuadMultiEncoder(
    (self_encoder): Sequential(
      (0): Linear(in_features=18, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): Tanh()
    )
    (feed_forward): Sequential(
      (0): Linear(in_features=256, out_features=512, bias=True)
      (1): Tanh()
    )
  )
  (actor_core): ModelCoreIdentity()
  (critic_encoder): QuadMultiEncoder(
    (self_encoder): Sequential(
      (0): Linear(in_features=18, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): Tanh()
    )
    (feed_forward): Sequential(
      (0): Linear(in_features=256, out_features=512, bias=True)
      (1): Tanh()
    )
  )
  (critic_core): ModelCoreIdentity()
  (actor_decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationContinuousNonAdaptiveStddev(
    (distribution_linear): Linear(in_features=512, out_features=4, bias=True)
  )
)
[2024-12-17 14:54:06,710][1080598] EvtLoop [learner_proc0_evt_loop, process=learner_proc0] unhandled exception in slot='init' connected to emitter=Emitter(object_id='Runner_EvtLoop', signal_name='start'), args=()
Traceback (most recent call last):
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/sample_factory/algo/learning/learner_worker.py", line 139, in init
    init_model_data = self.learner.init()
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/sample_factory/algo/learning/learner.py", line 215, in init
    self.actor_critic.model_to_device(self.device)
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/sample_factory/model/actor_critic.py", line 58, in model_to_device
    module.model_to_device(device)
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/sample_factory/model/encoder.py", line 24, in model_to_device
    self.to(device)
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
  File "/home_2/master_113/m16134012/miniconda3/envs/swarm-rl/lib/python3.8/site-packages/torch/cuda/__init__.py", line 314, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
[2024-12-17 14:54:06,712][1080598] Unhandled exception No CUDA GPUs are available in evt loop learner_proc0_evt_loop
[2024-12-17 14:54:06,748][1080607] Worker 1 uses CPU cores [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
[2024-12-17 14:54:06,787][1080606] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[2024-12-17 14:54:06,836][1080605] Using GPUs [0] for process 0 (actually maps to GPUs [3])
[2024-12-17 14:54:06,837][1080605] Set environment var CUDA_VISIBLE_DEVICES to '3' (GPU indices [0]) for inference process 0
[2024-12-17 14:54:06,860][1080605] Num visible devices: 0
[2024-12-17 14:54:24,899][1079745] Heartbeat connected on Batcher_0
[2024-12-17 14:54:24,905][1079745] Heartbeat connected on InferenceWorker_p0-w0
[2024-12-17 14:54:24,908][1079745] Heartbeat connected on RolloutWorker_w0
[2024-12-17 14:54:24,910][1079745] Heartbeat connected on RolloutWorker_w1
[2024-12-17 14:54:52,011][1079745] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 1079745], exiting...
[2024-12-17 14:54:52,011][1079745] Runner profile tree view:
main_loop: 47.1008
[2024-12-17 14:54:52,012][1079745] Collected {}, FPS: 0.0
[2024-12-17 14:54:52,012][1080607] Stopping RolloutWorker_w1...
[2024-12-17 14:54:52,012][1080606] Stopping RolloutWorker_w0...
[2024-12-17 14:54:52,012][1080605] Stopping InferenceWorker_p0-w0...
[2024-12-17 14:54:52,012][1080598] Stopping Batcher_0...
[2024-12-17 14:54:52,012][1080607] Loop rollout_proc1_evt_loop terminating...
[2024-12-17 14:54:52,012][1080598] Loop batcher_evt_loop terminating...
[2024-12-17 14:54:52,012][1080606] Loop rollout_proc0_evt_loop terminating...
[2024-12-17 14:54:52,012][1080605] Loop inference_proc0-0_evt_loop terminating...
